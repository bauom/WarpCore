# Roadmap / TODO

This document tracks upcoming milestones, tasks, and open questions for the **Inference Library**.

> Legend: âœ… Done Â· ğŸš§ In Progress Â· ğŸ’¤ Blocked Â· ğŸ“ Design/Planned Â· â“ Needs Discussion

---

## v0.1.0 (Current Focus)

- [âœ…] Core crate with traits, configs, error types
- [âœ…] OpenAI backend (chat completion API)
- [âœ…] Anthropic backend (messages API)
- [âœ…] Streaming generation API (`Stream<Item = Result<String>>`)
- [âœ…] Integration tests for OpenAI & Anthropic
- [âœ…] `diffusion-rs` backend (local text-to-image generation)
- [âœ…] `TextToImageModel` trait and core support for image generation.
- [ğŸš§] `llama.cpp` backend (CPU + GPU layers) - Basic implementation complete, needs refinement.
- [ğŸ“] CI setup (lints, tests, potentially ignored tests with secrets)
- [ğŸ“] Crate-level docs (`cargo doc --all-features`)
- [ğŸ“] Basic `tracing` instrumentation

---

## v0.2.0

- [ğŸ“] Configurable on-disk prompt cache (e.g., using Moka)
- [ğŸ“] Add usage statistics (token counts) to responses/events
- [ğŸ“] Helpers for auto-discovering local model files
- [ğŸ“] Improved error handling and reporting details
- [ğŸ“] Release initial version on crates.io

---

## v0.3.0

- [ğŸ“] `candle-rs` backend
- [ğŸ“] `groq` backend
- [ğŸ“] `modal` backend
- [ğŸ“] `ImageToTextModel` trait + OpenAI Vision implementation

---

## v0.4.0

- [âœ…] `TextToImageModel` trait + placeholder/basic implementation
- [ğŸ“] Prompt template library/integration
- [ğŸ“] Retry & back-off middleware for cloud backends
- [ğŸ“] More comprehensive `tracing`/OpenTelemetry support

---

## v1.0.0

- [ğŸ“] Stabilise public API & mark crates `1.0`
- [ğŸ“] Complete feature flags coverage & documentation
- [ğŸ“] Expanded regression test-suite (potentially with fixture models/mock servers)
- [ğŸ“] Benchmarks

---

## Nice-to-haves / Ideas

- [â“] WASM support for inference in browsers (WebGPU)
- [â“] Python bindings via PyO3
- [â“] `xtask` command for releasing & validation
- [â“] Plugin system for community backends

---

## Future Backends

- [ ] `candle-rs`
- [ ] `groq` / `modal`

## Core Improvements

- [ ] More detailed tracing/observability
- [âœ…] Support for `ImageToText` and `TextToImage` model types
- [ ] Enhanced configuration validation

## Backend Specific

### `llama_cpp` Backend
- [âœ…] Implement `list_available_models` by scanning `MODELS_PATH`.
- [ğŸš§] Implement advanced sampling strategies (beyond greedy).
- [ğŸš§] Add integration tests.
- [ğŸš§] Improve error handling.
- [â“] Investigate and document concurrency limitations (tests hang unless run with `--test-threads=1`).

### `diffusion-rs` Backend
- [â“] **Audit Thread Safety**: The `DiffusionRsModel` currently uses `unsafe impl Send + Sync` due to raw pointers in `diffusion-rs` internals. This needs careful review and an attempt to remove `unsafe` if possible, or confirm and document the safety implications. (See `crates/backends/diffusion-rs/README.md`)
- [ğŸ“] **Sampler Mapping**: The `map_from_diffusion_rs_sampler` is currently unused (commented out). Decide if it's needed for future features or remove it.
- [ğŸ“] **Progress Callbacks**: Expose `diffusion_rs` progress callbacks for a better user experience during long generation tasks, potentially via a streaming API for progress updates.
- [ğŸ“] **Output Formats**: While `ImageOutputFormat` exists, the backend currently only directly produces PNG files. Add support for returning raw bytes and other formats like JPEG/WEBP if `diffusion-rs` supports them or if we add conversion.
- [ğŸ“] **List Local Models**: Enhance `list_available_models` to scan `$DIFFUSION_MODELS_PATH` for local model files, similar to the `llama_cpp` backend.
- [ğŸ“] **BackendConfig**: Respect user-supplied `BackendConfig` for potential future options like device selection or thread counts if `diffusion-rs` exposes such controls.

## Known Issues

- See backend-specific items above.

Feel free to open issues or PRs to suggest additions / claim tasks! 